# Chat Query Endpoint Documentation

This document describes the `/chat/query` endpoint for the Instant-RAG FastAPI backend, which allows users to ask questions to their selected project (documents + emails).

## Endpoint Details

**URL**: `/chat/query`
**Method**: `POST`
**Description**: Send a question to the RAG system and get an answer with citations.

## Request Format

```json
{
  "project_id": "string",
  "question": "string",
  "top_k": 5
}
```

### Parameters

- `project_id` (required): The ID of the project to query
- `question` (required): The user's question
- `top_k` (optional, default: 5): Number of top chunks to retrieve

## Response Format

```json
{
  "answer": "LLM answer text",
  "citations": [
    {
      "doc_name": "document_name.pdf",
      "page_number": 3,
      "source_type": "pdf",
      "images_base64": ["base64_encoded_image_1", "base64_encoded_image_2"]
    },
    {
      "doc_name": "another_document.md",
      "page_number": null,
      "source_type": "markdown",
      "images_base64": null
    }
  ]
}
```

### Response Fields

- `answer`: The answer generated by the LLM
- `citations`: An array of citation objects, each containing:
  - `doc_name`: Name of the cited document
  - `page_number`: Page number in the document (null if not applicable)
  - `source_type`: Type of the source (pdf, markdown, image, email)
  - `images_base64`: Array of base64 encoded images (null if not available)

## Implementation Details

The endpoint works as follows:

1. Embeds the question using the local model BAAI/bge-small-en
2. Searches the rag_chunks table using pgvector with cosine similarity
3. Filters results by project_id
4. Gets top K matching chunks
5. Builds a context string with the retrieved chunks
6. Sends the prompt to Gemini 2.0 Flash API
7. Returns the answer with citations

## Error Handling

The endpoint includes comprehensive error handling:

- If the project doesn't exist, returns a 404 error
- If no chunks are found, returns a graceful message
- If there's an issue with the LLM API, returns a fallback message
- All other errors are caught and return a 500 error with details

## Testing the Endpoint

You can test the endpoint using the provided test script:

```bash
# Make sure you have a .env file with GEMINI_API_KEY set
python test_chat_query.py
```

To list available projects:

```bash
python list_projects.py
```

## Environment Variables

The endpoint requires the following environment variables:

- `GEMINI_API_KEY`: API key for Google's Gemini Flash 2.0 API

You can set these in your `.env` file or export them in your shell.

## Logging

The endpoint logs the following information for debugging:

- Raw LLM prompt
- Raw LLM response
- Any errors that occur during processing

Logs are stored in the `logs` directory.
